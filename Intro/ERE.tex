\chapter{Résumé étendu}
(Reste à traduire en Français)
\section*{Introduction}

\textit{Scientific Workflows} (\textit{SWfs}) enable scientists to easily express multi-step computational activities, such as load input data files, process the data, run analyses, and aggregate the results. The computational activities are related by dependencies. A SWf describes activities and the dependencies typically as a graph, where vertexes represent data processing activities and edges represent dependencies between them. 
As the computation in scientific experiments becomes complex and analyzes big amounts of data, SWfs are widely used in multiple domains, such as astronomy \cite{Deelman2008}, biology \cite{Ocana2012}, physics \cite{Ogasawara2011}, seismology \cite{Deelman2006z}, meteorology \cite{Woitaszek2011} and so on.

SWfs are often \textit{data-intensive}, \textit{i.e.} process, manage or produce huge amounts of data.
Managing and manipulating data-intensive SWfs with traditional programming tools (\textit{e.g.} code libraries, scripting languages) becomes very hard and impossible as complexity increases. 
Therefore, \textit{SWf Management Systems} (\textit{SWfMSs}) have been specifically developed to ease dealing with SWfs, which includes many aspects such as modeling, programming, debugging, and executing SWfs. 
SWfMSs generally generate provenance data during SWf execution.
Provenance data, which traces the execution of SWfs and the relationship between input data and output data, is sometimes more important than SWf execution itself.
In order to execute data-intensive SWfs within a reasonable time, SWfMSs exploit parallelism techniques with High Performance Computing (HPC) resources in a cluster, grid or cloud environment. 
Some existing SWfMSs, \textit{e.g.} Pegasus \cite{Deelman2005,Deelman2014}, Swift \cite{Zhao2007},
and Chiron \cite{Ogasawara2013}, are publicly available for SWf execution and management. 
However, most of them are designed for computing cluster or grid environments. 
In cloud environments, SWfMSs generally use the same approaches designed for computing clusters or grids, which are not optimized for cloud environments. 

By offering virtually infinite resources, diverse scalable services, stable service quality and flexible payment policies, 
the \textit{cloud} becomes an appealing solution for SWf execution. 
SWfMSs can be easily deployed in the cloud exploiting \textit{Virtual Machines} (\textit{VMs}). 
With a pay-as-you-go method, the users of clouds do not need to buy physical machines and the maintenance of the machines is ensured by cloud providers. 
Thus, cloud environments become interesting infrastructures for SWf execution.

A cloud is typically \textit{multisite}, which is made of several sites (or data centers), each with its own resources and data and is explicitly accessible to cloud users.
Because of low latency and proprietary issues, the data are generally stored at the cloud site where the data sources are located. 
As a consequence, the input data of a SWf can be geographically distributed. 
For instance, the social graph data of Facebook \cite{Bronson2013} and the data of ALICE project \cite{alice} are geographically distributed.
Since a SWf may process geographically distributed data, SWf execution should be adapted to a multisite cloud while exploiting distributed computing or storage resources beyond one cloud site. 
The existing approaches focus on the computing cluster, grid or a single site cloud environment, which leave space for executing SWfs in multisite cloud environments. 

This thesis has been prepared in the context of two scientific projects: Z-CloudFlow and
MUSIC (MUltiSite Cloud data management) with the main objective of efficiently executing data-intensive SWfs in a multisite
cloud, where each site has its own cluster, data and programs. This thesis contains $5$ main chapters: state of the art, SWf partitioning, VM provisioning, multi-objective scheduling and task scheduling with provenance support. It starts with an introduction chapter and ends with a conclusion chapter that summarizes the contributions and proposes future directions.

\section*{State of the Art} 

A SWf is the assembly of complex sets of scientific data processing activities with data
dependencies between them \cite{Deelman2009}. 
A SWf Management System (SWfMS) is an efficient tool to execute workflows and
manage data sets in various computing environments.
In order to execute a SWf in a given environment, a SWfMS typically generates a Workflow
Execution Plan (WEP), which is a program that captures optimization decisions and
execution directives, typically the result of compiling and optimizing a workflow, before
execution.
This section presents the exiting techniques of SWfs and SWfMSs, including functional architecture, parallelization techniques, analysis of different SWfMSs, the multisite cloud environment.

The functional architecture of a SWfMS can be layered as follows \cite{Deelman2005,Zhao2007,Altintas2004,Ogasawara2013}:
presentation, user services, WEP generation, WEP execution
and infrastructure. A user interacts with a SWfMS through the presentation layer
and realizes the desired functions at user services layer. 
The user services layer generally supports provenance data, which is the metadata that captures the derivation history of a dataset.
A SWf is processed at WEP generation layer to produce a WEP, which is executed at the WEP execution layer.
In order to reduce the execution time, the SWfs are generally executed in parallel.
The SWfMS accesses the physical resources through the infrastructure layer for SWf execution.

The parallel execution of SWfs includes parallelism and scheduling. SWf parallelism identifies the tasks that can be executed in parallel. There are two parallelism levels: coarse-grained parallelism and fine-grained parallelism.
Coarse-grained parallelism, which is performed at workflow level, can achieve parallelism by executing SWf fragments in parallel. A SWf fragment (or fragment for short) can be defined as a subset of activities and data dependencies of the original SWf, which is generated by SWf partitioning. Fine-grained parallelism realizes parallelism by executing different activities in parallel within a SWf or a SWf fragment. SWf scheduling is a process of allocating concrete tasks to computing resources (\textit{i.e.} computing nodes) to be executed during SWf execution \cite{Bux2013}. Scheduling methods can be static, dynamic or hybrid. Static scheduling generates a SP that allocates all the executable tasks to computing nodes before execution and the SWfMS strictly abides the SP during the whole SWf execution \cite{Bux2013}. It is efficient when the execution environment varies little during the SWf execution, and when the SWfMS has enough information about the computing and storage capabilities of the corresponding computers. Dynamic scheduling produces SPs that distribute and allocate executable tasks to computing nodes during SWf execution \cite{Bux2013}.
This kind of scheduling is appropriate for SWfs, in which the workload of tasks is difficult to estimate, or for environments where the capabilities of the computers vary a lot during execution. Both of static and dynamic scheduling have their own advantages
and they can be combined as a hybrid scheduling method to achieve better performance
than just using one or the other.

There are eight typical SWfMSs and a gateway framework: Pegasus,
Swift, Kepler, Taverna, Chiron, Galaxy, Triana \cite{Taylor2007}, Askalon \cite{Fahringer2007} and WS-PGRADE/gUSE \cite{Kacsuk2012}. 
Pegasus and Swift have excellent support for scalability and high-performance of data-intensive SWfs.
Pegasus, Swift, Kepler, Taverna and WS-PGRADE/gUSE are widely used in astronomy, biology, and so on while Galaxy can only execute bioinformatics SWfs.
All of them support fine-grained parallelism, dynamic scheduling and three of them (Pegasus, Kepler and WS-PGRADE/gUSE) support static scheduling. All the eight SWfMSs and the gateway framework support SWf execution in both grid and cloud environments.
Chiron exploits an algebraic approach \cite{Ozsu2011} to manage the parallel execution of data-intensive SWfs. It uses an algebraic data model to express all data as relations and represent SWf activities as algebraic expressions in the presentation layer. 
A relation contains sets of tuples composed of basic attributes.
An algebraic expression consists of algebraic
activities, additional operands, operators, input relations and output relations.
An algebraic activity contains a program or an SQL expression, and input
and output relation schemas. 
An additional operand is the side information for the algebraic expression, which can be relations or a set of grouping attributes.
There are six operators that can automatically transform activities to multiple tasks to execute.

There are important cases where SWfs will need to be executed at several cloud sites, \textit{e.g.} because the data accessed by the SWf is in different research groups' databases in different sites or because SWf execution needs more resources than those at a single site. Big cloud providers such as Microsoft and Amazon typically have multiple geographically distributed data centers located at different sites. 
In a multisite cloud, we can execute a SWf with the fine-grained parallelism or the coarse-grained parallelism at the multisite level.
SWf execution with the fine-grained parallelism is to schedule all the tasks into each cloud site. 
Although there are some existing scheduling approaches \cite{Duan2014,Ostermann2009a} for this method, they have no support for provenance data, which is important for SWf execution.
With the coarse-grained parallelism, a SWf is partitioned to fragments. Each fragment is scheduled at a specific site and its tasks are scheduled to the VMs at this site. 
Some methods \cite{Chen2012a, Chen2013} are proposed to enable SWf multisite execution by SWf partitioning, they generally focus on a single objective, \textit{i.e.} reducing execution time, with a storage constraint while multiple objectives remain a problem, \textit{e.g.} reducing both execution time and monetary cost. In addition, they generally have no support for dynamic VM provisioning in the cloud, which is critical for the execution in a cloud environment. 
Chiron is adapted to the cloud through its extension, Scicumulus \cite{Oliveira2010,Oliveira2012}, which supports dynamic computing provisioning \cite{Oliveira2012a}. However, this approach just focuses on a single-site cloud environment.

\section*{SWf Partitioning}
We tackle the problem of SWf partitioning problem in order to execute SWfs in a multisite cloud. Our main objective is to enable SWf execution in a multisite cloud by partitioning SWfs into fragments in order to reduce execution time while ensuring some activities executed at specific cloud sites.

There are basically two SWf partitioning techniques, \textit{i.e.} DAG partitioning and data partitioning. DAG partitioning transforms a DAG composed of activities into a DAG
composed of fragments while each fragment is a DAG composed of activities and dependencies.
Data partitioning divides the input data of a fragment generated by DAG
partitioning into several data sets, each of which is encapsulated in a newly generated
fragment. We focus on DAG partitioning in this work. There is a general partitioning technique, \textit{i.e.} activity encapsulation, which encapsulates each activity as a SWf fragment.

We propose three SWf partitioning methods, namely Scientist Privacy (SPr), Data Transfer Minimizing (DTM) and Computing Capacity Adaptation (CCA). SPr partitions SWfs by putting locking activities and its available following activities to a fragment, in order to better support execution monitoring under security restriction. DTM partitions SWfs with the consideration of locking activities while minimizing the volume of data to be transferred among different SWf fragments. CCA partitions SWfs according to the computing capacity of different cloud sites. This technique tries to put more activities to the fragment to be executed within a cloud site with bigger computing capacity. 
Our proposed partitioning techniques are suitable for different configurations of clouds in order to reduce SWf execution time. In addition, we also propose to use data refining techniques, namely, file combining and data compression, to reduce the time to transfer data among different sites.

We take Buzz SWf as a use case and adapt the Chiron SWfMS for multisite SWf execution. We evaluate extensively our proposed partitioning techniques by executing the Buzz SWf with the adapted Chiron deployed at two sites, \textit{i.e.} Western Europe and Eastern US, of the Azure cloud.
We consider two cloud configurations, namely homogeneous configuration and heterogeneous configuration. All the sites have the same amounts and types of VMs correspond to the homogeneous configuration while the sites have different amounts or types of VMs correspond to the heterogeneous configuration. The experimental results show that DTM with data refining techniques is suitable ($24.1$\% of time saved compared to CCA without data refining) for executing SWfs in a multisite cloud with a homogeneous configuration, and that CCA performs better ($28.4$\% of time saved compared to SPr technique without data refining) with a heterogeneous configuration. In addition, the results also reveal that data refining techniques can significantly reduce the time to transfer data between two different sites. 

\section*{VM Provisioning}
We handle the problem of generating VM provisioning plans for SWf execution within a single cloud site for multiple objectives. Our main contribution is to propose a cost model and an algorithm in order to generate VM provisioning plans to reduce both execution time and monetary cost for SWf execution in a single site cloud.

To address the problem, we design a multi-objective cost model for the execution of SWfs within a single cloud site. The cost model is a weighted function with the objectives of reducing execution time and monetary cost based on user defined desired execution time and monetary cost. Our cost model takes the sequential workload and the cost to start VMs into consideration, which is more precise compared with existing cost models, \textit{e.g.} GraspCC \cite{Coutinho2014}. The execution time is estimated based on the Amdahl's law \cite{Sun2013} and the monetary cost is estimated based on the estimated execution time and the cost to use VMs. 

Then, based on the cost model, we propose a Single Site VM Provisioning (SSVP) algorithm to generate provisioning plans for SWf execution within a single cloud site. The SSVP first calculates an optimal number of CPU cores for SWf execution based on the multi-objective cost model. Then, it generates a provisioning plan and iteratively improves the provisioning plan in order to reduce the cost based on the cost model and the optimal number of CPU cores. Finally, SSVP generates a VM provisioning plan corresponding to the minimum cost to execute SWfs with the specific importance of each objective.

We make extensive evaluations to compare our cost model and algorithm with GraspCC. We execute SciEvol with different amounts of input data and different weights of execution time. We use the Chiron SWfMS at the Japan East site of Azure cloud. The experimental results show that our algorithm can adapt VM provisioning plans to different configurations, \textit{i.e.} generate different VM provisioning plans for different weights of execution time. SSVP generates better ($53.6\%$) VM provisioning plans compared with GraspCC. The results also reveal that our cost model is more ($76.7\%$) precise to estimate the execution time and the monetary cost compared with GraspCC, because of the consideration of the sequential workload and the cost to start VMs.

\section*{Multi-Objective Scheduling}

We address the problem of SWf fragment scheduling for multiple objectives in order to enable SWf execution in a multisite cloud with a stored data constraint. We assume that the data stored at a specific site may not be allowed to be transferred to other sites because of proprietary or big amounts of data, which is denoted as stored data constraint. In this work, we took into consideration of different prices to use VMs and stored data at different sites.

The multisite cost model is a weighted function composed of execution time and monetary cost based on user defined desired execution time and monetary cost. However, since it is difficult to estimate the global execution and monetary cost to execute a SWf in a multisite cloud, we propose our multisite cost model as a combination of the cost to execute each SWf fragment. We also decompose the desired execution time and monetary cost to the that of each SWf fragment. Based on the desired execution time and monetary cost of each SWf fragment, we can estimate the cost to execute each SWf fragment at a scheduled site based on the cost model corresponding to SSVP. Finally, our multisite cost model can estimate the global cost with the consideration of the cost to transfer data across different sites with a scheduling plan.

We present two adapted scheduling algorithms, \textit{i.e.} data location based scheduling (LocBased) and site greedy scheduling (SGreedy), and our proposed algorithm, namely activity greedy scheduling (ActGreedy). LocBased exploits DTM to partition SWfs and schedules the fragments to the site where its input data is stored. This algorithm does not take the monetary cost into consideration and may incur a big cost to execute SWfs. SGreedy takes advantage of the activity encapsulation technique to partition SWfs and schedules the best fragment to each site. SGreedy schedules the activities of a pipeline of activities to different sites, which leads to bigger intersite data transfer and execution time. ActGreedy partitions SWfs with the activity encapsulation technique and groups small fragments to bigger fragments to reduce data transfer among different sites and schedules each fragment to the best site. This algorithm can reduce the overall execution time by comparing the cost to execute fragments at each site, which is generated based on the SSVP algorithm. 

We evaluated our scheduling algorithm by executing SciEvol with different amounts of input data and different weights of objectives at three cloud sites of Azure. The three cloud sites are West Europe, Japan West, and Japan East and the costs of using VMs at each site are different. We used SSVP to generate VM provisioning plans and the Chiron SWfMS to execute SWf fragments at each site.
The experimental results shown that ActGreedy performs better in terms of the weighted cost to execute SWfs in a multisite cloud compared with LocBased (up to $10.7\%$) and SGreedy (up to $17.2\%$). In addition, the results also reveal that the scheduling time of ActGreedy is reasonable compared with two general approaches, \textit{i.e.} Brute force and Genetic.

\section*{Task Scheduling with Provenance Support}

We dealt with task scheduling problem for multisite SWf execution with provenance support. The main goal was to enable SWf execution with the distributed input data at different sites within the minimum time with provenance support while the bandwidths among different sites are different. In this work, we propose the multisite Chiron and a task scheduling algorithm.

Multisite Chiron is an extension of Chiron for multisite cloud environments. Chiron implements an algebraic approach to express SWfs, optimize SWf execution in a single cluster. Multisite Chiron enables task execution of an activity at different sites to process the distributed data simultaneously. We also propose the multisite provenance model for multisite Chiron. In a multisite cloud, we propose different data communication methods for the intersite data transfer. We use our two level scheduling method, \textit{i.e.} multisite scheduling and single site scheduling, for task scheduling in a multisite cloud.  Multisite Chiron corresponds to fine-grained parallelism at the multisite level, which is different from the coarse-grained parallelism. The fine-grained parallelism enables different tasks of one fragment to be executed at different cloud sites.

We propose the Data-Intensive Multisite task scheduling (DIM) algorithm to schedule tasks at multisite level with the consideration of the support for provenance data, different bandwidths among different sites and the distribution of input data. First, DIM schedules the tasks according to the data location at different sites. Then, DIM reschedules the tasks in order to achieve load balance among different cloud sites based on a cost model to estimate the execution of each site. The load balance represents that it takes the same time to execute the tasks at each site. The cost model considers the time to transfer the input data of the tasks across different sites and the time to transfer the provenance data to a centralized provenance database.

We evaluated our algorithms and multisite Chiron by executing Buzz and Montage in three Azure cloud sites, \textit{i.e.} Central US, West Europe and North Europe. We executed Buzz with different amounts of input data and Montage with different degrees using the multisite Chiron. The experimental results show that DIM performs much better than two existing scheduling algorithms, \textit{i.e.} MCT (up to $24.3\%$) and OLB (up to $49.6\%$), in terms of execution time. Moreover, DIM can also reduce significantly (up to more than $7$ times) data transfer between sites, compared with MCT and OLB. In addition, the results also reveal that the scheduling time of DIM is reasonable compared with the overall execution time of SWfs (less than $3$\%). In particular, the experiments show that the distribution of tasks is adapted according to different bandwidths among different sites for the generation of provenance data.

\section*{Conclusion}

In this thesis, we addressed the problem of executing data-intensive SWfs in a multisite cloud, where the data and computing resources may be distributed at different cloud sites. 
To this end, we proposed a distributed and parallel approach that leverages the resources available at different cloud sites, based on a survey of existing techniques.
In the survey, we proposed a functional SWfMS architecture by analyzing and categorizing the existing techniques. 
To exploit parallelism, we used an algebraic approach, which allows expressing SWf activities using operators and automatically transforming them into multiple tasks.
We proposed SWf partitioning algorithms, a dynamic VM provisioning algorithm, an activity scheduling algorithm and a task scheduling algorithm. 
Different SWf partitioning algorithms partition a SWf to several fragments. 
The VM provisioning algorithm is used to dynamically create an optimal combination of VMs for executing SWf fragments at each cloud site. 
The activity scheduling algorithm distributes the SWf fragments to the cloud sites with the minimum cost based on a multi-objective cost model. 
The task scheduling algorithm directly distributes tasks among different cloud sites while achieving load balancing at each site based on a multisite SWfMS. 
We evaluated our proposed solutions by executing real-life SWfs in the Microsoft Azure cloud, the results of which shown the advantages of our solutions over the existing techniques.

Our contributions can be used as a starting point for future research. We propose the following future research directions:
\begin{itemize}
\item \textbf{Provenance distribution}: We believe that distributed provenance data management can reduce the time to generate or retrieve data at each site in order to reduce the overall SWf execution time in a multisite cloud. 
\item \textbf{Data transfer}: One possible solution to efficiently transfer data between two cloud sites is to select several nodes at each site to send or receive data, by exploiting parallel data transfer and making the data transfer more efficient. 
\item \textbf{Multisite Spark}: We believe that it is interesting to propose multisite scheduling algorithms and optimizations to use Spark for multisite SWf execution. 
\item \textbf{Architecture}: A peer-to-peer architecture can be used in order to achieve fault-tolerance during the execution of SWfs both within a single site cloud or a multisite cloud. 
\item \textbf{Dynamic Scheduling}: We believe that dynamic scheduling of activities or tasks can perform better with the parameters measured during SWf execution in terms of execution time, monetary cost, energy consumption, and others for SWf execution in a multisite cloud. 
\end{itemize}

\section*{Publications}

All the contributions have been published in the following publications:

\begin{itemize}

\item \textit{Ji Liu}. Multisite Management of Data-intensive Scientific Workflows in the Cloud. In BDA’$2014$: Gestion de données - principles, technologies et applications, $2014$, $28-30$.

\item \textit{Ji Liu}, Esther Pacitti, Patrick Valduriez, Vitor Silva Souza, Marta Mattoso. Scientific Workflow Partitioning in Multi-site Clouds. In BigDataCloud'2014: 3rd Workshop on Big Data Management in Clouds in conjunction with Euro-Par, Aug $2014$. Springer, Lecture Notes in Computer Science, $8805$,  $105-116$.

\item \textit{Ji Liu}, Esther Pacitti, Patrick Valduriez, Marta Mattoso, Parallelization of Scientific Workflows in the Cloud, Research Report RR-$8565$, $2014$.

\item \textit{Ji Liu}, Esther Pacitti, Patrick Valduriez, Marta Mattoso. A Survey of Data-Intensive Scientific Workflow Management. In Journal of Grid Computing, $2015$, volume $13$, number $4$, $457-493$.

\item \textit{Ji Liu}, Esther Pacitti, Patrick Valduriez, Daniel Oliveira, Marta Mattoso. Multi-objective scheduling of Scientific Workflows in multisite clouds. In Future Generation Computer Systems, $2016$, volume $63$, $76-95$.

\item \textit{Ji Liu}, Esther Pacitti, Patrick Valduriez, Marta Mattoso. Scientific Workflow Scheduling with Provenance Support in Multisite Cloud. In 12th International Meeting on High Performance Computing for Computational Science (VECPAR), $2016$, $1-8$.

\item \textit{Ji Liu}, Esther Pacitti, Patrick Valduriez, Daniel de Oliveira and Marta Mattoso. Multi-Objective Scheduling of Scientific Workflows in Multisite Clouds. In BDA’$2016$: Gestion de données - principles, technologies et applications, $2016$. To appear.

\end{itemize}
