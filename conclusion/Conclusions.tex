\chapter{Conclusions} \label{ch:conclusion}

In this thesis, we addressed the problem of executing data-intensive SWfs in a multisite cloud, where the data and computing resources may be distributed in different cloud sites. 
To this end, we proposed a distributed and parallel approach that leverages the resources available at different cloud sites, based on a survey of existing techniques.
In the survey, we proposed a functional SWfMS architecture by analyzing and categorizing the existing techniques. 
To exploit parallelism, we used an algebraic approach, which allows expressing SWf activities using operators and automatically transforming them into multiple tasks.
We proposed SWf partitioning algorithms, a dynamic VM provisioning algorithm, an activity scheduling algorithm and a task scheduling algorithm. 
Different SWf partitioning algorithms partition a SWf to several fragments. 
The VM provisioning algorithm is used to dynamically create an optimal combination of VMs for executing SWf fragments at each cloud site. 
The activity scheduling algorithm distributes the SWf fragments to the cloud sites with the minimum cost based on a multi-objective cost model. 
The task scheduling algorithm directly distributes tasks among different cloud sites while achieving load balancing at each site based on a multisite SWfMS. 
We evaluated our proposed solutions by executing real-life SWfs in the Microsoft Azure cloud, the results of which shown the advantages of our solutions over the existing techniques.
In this chapter, we summarize and discuss the contributions made in this thesis. Then, we give some research directions for future work.

\section{Contributions}

\begin{bf}A survey of existing techniques for SWfs execution.\end{bf} 
\\[6pt]
\noindent We discussed the existing techniques for parallel execution of data-intensive SWfs in different infrastructures, especially in the cloud. 
First, we introduced the definitions in SWf management, including SWfs and SWfMSs. Then, we presented in more details a five-layer functional architecture of SWfMSs and the corresponding functions. Special attention has been paid to data-intensive SWfs by identifying their features and presenting the corresponding techniques.
Second, we presented the basic techniques for the parallel execution of SWfs in SWfMSs: parallelization and scheduling. 
We showed how different kinds of parallelism (coarse-grained parallelism, data parallelism, independent parallelism and pipeline parallelism) can be exploited for
parallelizing SWfs. The scheduling methods to allocate tasks to computing resources can be static or dynamic, with
different trade-offs, or hybrid to combine the advantages of static and dynamic scheduling methods.
SWf scheduling may include an optimization phase to minimize a multi-objective function, in a given context (cluster, grid, cloud).
Third, we discussed cloud computing and the basic techniques for parallel execution of SWfs in the cloud, 
including single site cloud and multisite cloud. We discussed three categories of cloud computing, multisite management in the cloud and data storage in the cloud. The data storage techniques include shared-disk file systems and distributed file systems. Then, we analyzed the parallelization techniques of SWfs in both single site cloud and multisite cloud. 
Fourth, to illustrate the use of the techniques, we introduced the recent parallelization frameworks such as MapReduce and gave a comparative analysis of eight popular SWfMSs (Pegasus, Swift, Kepler, Taverna, Chiron, Galaxy, Triana and Askalon) and a science gateway framework (WS-PGRADE/gUSE).
Finally, we identified the limitations of existing techniques and proposed some research issues.
\\[12pt]
\noindent \begin{bf}SWf Partitioning for the Execution in a Multisite Cloud.\end{bf} 
\\[6pt]
\noindent We tackled the problem of SWf partitioning problem in order to execute SWfs in a multisite cloud. Our main objective was to enable SWf execution in a multisite cloud by partitioning SWfs into fragments while ensuring some activities executed at specific cloud sites.
First, we presented the general SWf partitioning techniques, \textit{i.e.} data partitioning and DAG partitioning. Then, we focused on DAG partitioning and mentioned activity encapsulation technique. Afterward, we proposed our SWf partitioning methods, namely Scientist Privacy (SPr), Data Transfer Minimization (DTM) and Computing Capacity Adaptation (CCA). SPr partitions SWfs by putting locking activities and its available following activities to a fragment, in order to better support execution monitoring under security restriction. DTM partitions SWfs with the consideration of locking activities while minimizing the volume of data to be transferred among different SWf fragments. CCA partitions SWfs according to the computing capacity of different cloud sites. This technique tries to put more activities to the fragment to be executed within a cloud site with bigger computing capacity. 
Our proposed partitioning techniques are suitable for different configurations of clouds in order to reduce SWf execution time. In addition, we also proposed to use data refining techniques, namely, file combining and data compression, to reduce the time to transfer data among different sites.

We evaluated extensively our proposed partitioning techniques by executing the Buzz SWf at two sites, \textit{i.e.} Western Europe and Eastern US, of the Azure cloud with different configurations. All the sites have the same amounts and types of VMs correspond to the homogeneous configuration while the sites have different amounts or types of VMs correspond to the heterogeneous configuration. 
The experimental results show that DTM with data refining techniques is suitable ($24.1$\% of time saved compared to CCA without data refining) for executing SWfs in a multisite cloud with a homogeneous configuration, and that CCA performs better ($28.4$\% of time saved compared to SPr technique without data refining) with a heterogeneous configuration. In addition, the results also reveal that data refining techniques can significantly reduce the time to transfer data between two different sites. 
\\[12pt]
\noindent \begin{bf}VM Provisioning in a Single Site Cloud.\end{bf} 
\\[6pt]
\noindent We handled the problem of generating VM provisioning plans for SWf execution within a single cloud site for multiple objectives. Our main contribution was to propose a cost model and an algorithm in order to generate VM provisioning plans to reduce both execution time and monetary cost for SWf execution in a single site cloud.
To address the problem, we designed a multi-objective cost model for the execution of SWfs within a single cloud site. The cost model is a weighted function with the objectives of reducing execution time and monetary cost. Our cost model takes the sequential workload and the cost to start VMs into consideration, which is more precise compared with existing cost models. Then, based on the cost model, we proposed Single Site VM Provisioning (SSVP) algorithm to generate provisioning plans for SWf execution within a single cloud site. The SSVP first calculates an optimal number of CPU cores for SWf execution. Then, it generates a provisioning plan and iteratively improves the provisioning plan in order to reduce the cost based on the cost model and the optimal number of CPU cores. 

We made extensive evaluations to compare our cost model and algorithm with an existing approach, \textit{i.e.} GraspCC. We executed SciEvol with different amounts of input data and different weights of execution time at the Japan East site of Azure cloud. The experimental results show that our algorithm can adapt VM provisioning plans to different configurations, \textit{i.e.} different weights of execution time and generate better ($53.6\%$) VM provisioning plans compared with GraspCC. The results also reveal that our cost model is more ($76.7\%$) precise to estimate the execution time and the monetary cost compared with the existing approach.
\\[12pt]
\noindent \begin{bf}Multi-Objective SWf Fragment Scheduling in a Multisite cloud.\end{bf} 
\\[6pt]
\noindent We addressed the problem of SWf fragment scheduling for multiple objectives in order to enable SWf execution in a multisite cloud with a stored data constraint. In this work, we took into consideration of different prices to use VMs and stored data at different sites.
We formally defined the scheduling problem of executing SWfs in a multisite cloud for multiple objectives with the stored data constraint. Then, we presented the system model for multisite SWf execution. Afterward, we detailed our multi-objective cost model composed of a weighted function with two objectives, \textit{i.e} reducing execution time and monetary cost. In addition, the cost model considers different costs of using VMs at different cloud sites. Finally, we presented two adapted scheduling algorithms, \textit{i.e.} data location based scheduling (LocBased) and site greedy scheduling (SGreedy), and our proposed algorithm, namely activity greedy scheduling (ActGreedy). LocBased exploits DTM to partition SWfs and schedules the fragments to the site where its input data is stored. This algorithm does not take the monetary cost into consideration and may incur a big cost to execute SWfs. SGreedy takes advantage of the activity encapsulation technique to partition SWfs and schedules the best fragment for each site. SGreedy schedules the activities of a pipeline of activities to different sites, which leads to bigger intersite data transfer and execution time. ActGreedy partitions SWfs with the activity encapsulation technique and groups small fragments to bigger fragments to reduce data transfer among different sites and schedules each fragment to the best site. This algorithm can reduce the overall execution time by comparing the cost to execute fragments at each site, which is generated based on the SSVP algorithm. 

We evaluated our scheduling algorithm by executing SciEvol with different amounts of input data and different weights of objectives at three cloud sites of Azure. The three cloud sites are West Europe, Japan West, and Japan East and the costs of using VMs at each site are different. We used SSVP to generate VM provisioning plans to execute SWf fragments at each site.
The experimental results shown that ActGreedy performs better in terms of the weighted cost to execute SWfs in a multisite cloud compared with LocBased (up to $10.7\%$) and SGreedy (up to $17.2\%$). In addition, the results also reveal that the scheduling time of ActGreedy is reasonable compared with two general approaches.
\\[12pt]
\noindent \begin{bf}Multisite Chiron and Multisite Task Scheduling with Provenance Support.\end{bf} 
\\[6pt]
\noindent We dealt with task scheduling problem for multisite SWf execution with provenance support. The main goal was to enable SWf execution with the distributed input data at different sites within the minimum time with provenance support while the bandwidths among different sites are different. In this work, we formally defined the task scheduling problem for multisite SWf execution, including the support for provenance data, different bandwidths among different sites and the distribution of input data. Then, we proposed multisite Chiron, which enables scheduling and executing tasks of one activity at different sites with a centralized provenance database. We also detailed the modifications made to adapt single site Chiron to multisite. Then, we proposed our two level scheduling and our intersite task scheduling algorithm, \textit{i.e.} Data-Intensive Multisite task scheduling (DIM). DIM considers the data locality and different bandwidths among different sites while transferring intersite provenance data. DIM also achieves load balance among different sites for the task execution based on an execution time estimation method.

We evaluated our algorithms and multisite Chiron by executing Buzz and Montage in three Azure cloud sites, \textit{i.e.} Central US, West Europe and North Europe. We executed Buzz with different amounts of input data and Montage with different degrees using the multisite Chiron. The experimental results show that DIM performs much better than two existing scheduling algorithms, \textit{i.e.} MCT (up to $24.3\%$) and OLB (up to $49.6\%$), in terms of execution time. Moreover, DIM can also reduce significantly (up to more than $7$ times) data transfer between sites, compared with MCT and OLB. In addition, the results also reveal that the scheduling time of DIM is reasonable compared with the overall execution time of SWfs (less than $3$\%). In particular, the experiments show that the distribution of tasks is adapted according to different bandwidths among different sites for the generation of provenance data.

\section{Directions for Future Work}

Our contributions can be used as a starting point for future research. With \textit{big data} being produced and to be processed at different sites of the cloud, multisite management of SWf execution in the cloud becomes more and more important. Based on this thesis' results, we can propose some future research directions:
\begin{itemize}
\item \textbf{Provenance distribution.} The existing SWfMSs generally store the provenance data in a centralized way and stores the data at a centralized site. This may incur network congestion when large amounts of tasks are executed in parallel. Thus, we believe that distributed provenance data management can reduce the time to generate or retrieve data at each site in order to reduce the overall SWf execution time in a multisite cloud. In addition, with data replication approaches in the distributed provenance data architecture, we believe that this approach can also help fault-tolerance at the multisite level, \textit{i.e.} the situation where a cloud site is down.
\item \textbf{Data transfer.} The data transfer between two sites is generally achieved by
having two nodes, each at one of the two sites, communicating directly. This method is not efficient in a multisite cloud. One possible solution is to select several nodes at each site to send or receive data, by exploiting parallel data transfer and making the data transfer more efficient. Then, the problem of matching the nodes at two cloud sites is critical for the multisite data transfer rate. 
\item \textbf{Multisite Spark.} Because of in-memory data processing, Spark has become a major framework for big data processing. Spark can be used as an engine to execute SWfs. However, Spark is designed for a single cluster environment, where the bandwidth among different computing nodes are high and similar. Thus, it is interesting to propose multisite scheduling algorithms and optimizations to use Spark for multisite SWf execution. In addition, the problem of VM provisioning for Spark clusters in the cloud also remains critical to use Spark in a multisite cloud.
\item \textbf{Architecture.} The structure of SWfMSs is generally centralized, with a master node, which is a single point of failure and performance bottleneck, managing all the optimization and scheduling processes. In addition, the system models (see Chapter \ref{SWPMC}, \ref{MOSSWMC}, \ref{SWSPSMC}) presented in this thesis are based on a master-worker model. A peer-to-peer architecture can be used in order to achieve fault-tolerance during the execution of SWfs both within a single site cloud or a multisite cloud. With a peer-to-peer architecture and data replication approaches, we believe the multisite SWfMSs are robust enough to address the situation where one computing node is down or even a cloud site is down.
\item \textbf{Dynamic Scheduling.} The scheduling algorithms generally schedule activities or tasks according to predefined parameters with a static scheduling approach. However, the workload or the features of VMs are dynamically varying at each site of the cloud. We believe that dynamic scheduling of activities or tasks can perform better with the parameters measured during SWf execution in terms of execution time, monetary cost, energy consumption, and others for SWf execution in a multisite cloud. 
\end{itemize}



